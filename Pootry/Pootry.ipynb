{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pootry.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PEdV1L1xveSN"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEdV1L1xveSN"
      },
      "source": [
        "# Pootry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XW9gbY_pzt36"
      },
      "source": [
        "`Richard Rivaldo 13519185`\n",
        "\n",
        "`Informatics Engineering Institut Teknologi Bandung`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJLU5kAfvc6o"
      },
      "source": [
        "**Pootry** is a Natural Language Processing project on building a `Deep Learning` model of poetry generator. The planned model for this project will be the `Bidirectional LSTM` alias `Long Short-Term Memory`, also with `GloVe Embeddings` or `Global Vectors` for words embeddings technique used in the training phase. Let's see how it goes!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWf-bpsp4gZB"
      },
      "source": [
        "The repository of the project: [Pootry](https://github.com/RichardRivaldo/Pootry).  The dataset can also be found in the repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWsSQXSL1wUP"
      },
      "source": [
        "# Preparations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XTnkbRFzfbZ"
      },
      "source": [
        "The dataset used in this project is a `.txt` file containing over 2000 verses (verses? lines? whatever~). For GloVe embeddings, this number might seem a little few compared to the pre-trained vectors made with it. Nonetheless, I still want to create my very first own embeddings as the sole purpose of this project is to gain deeper understanding about Natural Language Processing`s techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4Xiogsz8Auy"
      },
      "source": [
        "**Library Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCC7uXiK7_OM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9413d95-520b-491d-a5c4-f6557753822d"
      },
      "source": [
        "# Install Glove and Tensorflow Text\n",
        "! pip install glove-python-binary\n",
        "! pip install tensorflow_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: glove-python-binary in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from glove-python-binary) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from glove-python-binary) (1.4.1)\n",
            "Requirement already satisfied: tensorflow_text in /usr/local/lib/python3.7/dist-packages (2.5.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: tensorflow<2.6,>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (2.5.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub>=0.8.0->tensorflow_text) (3.12.4)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub>=0.8.0->tensorflow_text) (1.19.5)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (1.15.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (1.12)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (1.1.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (0.4.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (1.1.2)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (1.12.1)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (3.7.4.3)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (2.5.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (2.5.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (0.36.2)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6,>=2.5.0->tensorflow_text) (1.34.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow-hub>=0.8.0->tensorflow_text) (57.0.0)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow<2.6,>=2.5.0->tensorflow_text) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (0.4.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (1.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (0.6.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (4.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (2021.5.30)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (4.7.2)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (3.4.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow_text) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "708s_73O85rI"
      },
      "source": [
        "# Import all libraries\n",
        "import re\n",
        "import warnings\n",
        "import numpy as np\n",
        "from glove import Corpus, Glove\n",
        "import tensorflow_text as tftext\n",
        "from tensorflow.ragged import constant\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.initializers import Constant\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Embedding, Dropout, Bidirectional, LSTM, Dense, InputLayer, LeakyReLU\n",
        "\n",
        "# Let's ignore warning, shall we?\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpAjGnZGD-xg"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqMt1S1qEGld"
      },
      "source": [
        "**Reading and Cleaning the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DxsngsxEJj2"
      },
      "source": [
        "# Data path for Colab\n",
        "data_path = \"/content/poems.txt\"\n",
        "\n",
        "# Tried using Shakespeeare's. \n",
        "# It crashed due to massive number of tokens when doing one-hot encoding. :("
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rbm-RB_Qcyym"
      },
      "source": [
        "# Function to clean the verses\n",
        "# Simply replaces punctuations with empty char\n",
        "def clean_verses(list_of_verses):\n",
        "  # Clean all punctuations\n",
        "  # Except for ' and - that might have meanings for the word using it\n",
        "  cleaned_verse = [re.sub(r'[\\!\"#$%&\\*+,./:;<=>?@^_`()|~=]', \"\", verse) for verse in list_of_verses]\n",
        "  \n",
        "  return cleaned_verse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqhZx6SWZYkZ"
      },
      "source": [
        "# Read the file and store each line into a list of lists of tokenized words in every verses\n",
        "def read_poems_data(data_path, clean_puncts=False):\n",
        "  # Read the file and split the verses by newline\n",
        "  # Also filter out the empty strings\n",
        "  with open(data_path) as poems:\n",
        "    verses = poems.read()\n",
        "    verses = verses.split(\"\\n\")\n",
        "    verses = list(filter(lambda verse: verse != \"\", verses))\n",
        "  \n",
        "  # If the data wants to be cleaned, the clean_puncts can be changed to True\n",
        "  if clean_puncts:\n",
        "      verses = clean_verses(verses)\n",
        "  \n",
        "  return verses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nL396dBUnC1N"
      },
      "source": [
        "**Tokenize the Verses**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLOPp5fyZlLs"
      },
      "source": [
        "# Tokenize the words in each verse\n",
        "def tokenize_verses(verses):\n",
        "  all_tokens = []\n",
        "  for verse in verses:\n",
        "    # Use regex to split the strings based on whitespace\n",
        "    # Also based on -- (not -) because there are many words using it\n",
        "    tokens = re.split(r'--| ', verse)\n",
        "\n",
        "    # Iterate over every tokens\n",
        "    # If a word is UPPERCASE, change it to sentence case for better dictionary\n",
        "    tokens = [token.capitalize() if token.isupper() else token for token in tokens]\n",
        "\n",
        "    # Filter empty string from the second split\n",
        "    tokens = list(filter(lambda token: token != \"\", tokens))\n",
        "    all_tokens.append(tokens)\n",
        "\n",
        "  return all_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq5-zfXeqT4L"
      },
      "source": [
        "# GlOVe (Global Vectors) Words Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cr4RKhyt02a"
      },
      "source": [
        "**Create Corpus**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkZkHY9tt6xP"
      },
      "source": [
        "def create_corpus(tokens, window=20):\n",
        "  # Instantiate the corpus\n",
        "  corpus = Corpus()\n",
        "\n",
        "  # Create the occurence matrix with context window of 20\n",
        "  # Context window is the technique of counting co-occurence\n",
        "  # 20 means that we will count co-occurence 20 words left-right\n",
        "  # The number is chosen because average maximum words for the dataset is less than 15\n",
        "  # Fit the verses into the corpus\n",
        "  corpus.fit(tokens, window)\n",
        "  \n",
        "  return corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEMNtX7Due7e"
      },
      "source": [
        "**GloVe Embedding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br6OPzBXqcB3"
      },
      "source": [
        "# Embed the corpus into GloVe Model\n",
        "# Components are the numbers of latent vector dimension\n",
        "# Learning Rate is the SGD Learning Rate\n",
        "# Epochs is the number of training epochs for fitting the corpus\n",
        "# Number of threads is the number of threads used in training the data\n",
        "def embed_glove(corpus, num_of_components=300, lr=0.05, epochs=100, num_of_threads=30):\n",
        "  # Instantiate the model\n",
        "  glove = Glove(no_components=num_of_components, learning_rate=lr)\n",
        "\n",
        "  # Fit over the co-occurence matrix in the corpus\n",
        "  glove.fit(corpus.matrix, epochs=epochs, no_threads=num_of_threads)\n",
        "\n",
        "  # Add the vocab of corpus to the model\n",
        "  glove.add_dictionary(corpus.dictionary)\n",
        "\n",
        "  return glove"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9Oei_7-BaYv"
      },
      "source": [
        "**Testing the Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_ulYr7uxK4K"
      },
      "source": [
        "# Find the 10 most similar word\n",
        "# glove_embeddings.most_similar(\"devil\", number=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8qdY-EdA94M"
      },
      "source": [
        "# Sequence Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUKjwFhXBymy"
      },
      "source": [
        "**Construct Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awNPpwBKCnUt"
      },
      "source": [
        "# Using Tensorflow Keras Tokenizer API\n",
        "# The tokenizer will take all preprocessed tokens from before\n",
        "def construct_tokenizer(tokens, lower_text=False):\n",
        "  # Create the tokenizer with specialized Out of Vocabulary Token\n",
        "  tokenizer = Tokenizer(lower=lower_text, oov_token=\"<OOV>\")\n",
        "\n",
        "  # Fit the tokenizer into the tokens\n",
        "  tokenizer.fit_on_texts(tokens)\n",
        "\n",
        "  return tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLCN1WLI_p9A"
      },
      "source": [
        "**Rejoin Sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEdeTqd63MyE"
      },
      "source": [
        "# Rejoin all tokens back into a sentence\n",
        "def rejoin_sentences(tokens):\n",
        "  # Join all tokens in a sentence and put it in this list\n",
        "  return [' '.join(tokens_sentence) for tokens_sentence in tokens]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYovHngq2XDN"
      },
      "source": [
        "**Sequence Modelling of the Words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Hd77VaB5GrV"
      },
      "source": [
        "# Convert the sentences into sequences\n",
        "# Will generate sequences of iteratively increasing N-Grams form of the sequence\n",
        "def generate_sequences(tokenizer, sentences):\n",
        "  n_grams_sequences = []\n",
        "\n",
        "  # Iterate over each sentence and convert it into sequences with the tokenizer\n",
        "  for sentence in sentences:\n",
        "    seq = tokenizer.texts_to_sequences([sentence])[0]\n",
        "\n",
        "    # Iterate over the sequence made from before and build N Gram sequences iteratively\n",
        "    for idx in range(1, len(seq)):\n",
        "      # Minimum length of the sequence will be two converted token\n",
        "      n_gram_seq = seq[: idx + 1]\n",
        "      n_grams_sequences.append(n_gram_seq)\n",
        "  \n",
        "  return n_grams_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ1gw7QABgLd"
      },
      "source": [
        "**Generate Embeddings Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZjXPAleBqGZ"
      },
      "source": [
        "def generate_embed_matrix(embeddings, tokenizer):\n",
        "  # Get the word index and number of tokens from the tokenizer\n",
        "  word_index = tokenizer.word_index\n",
        "  # Already included the OOV word\n",
        "  num_of_tokens = len(word_index)\n",
        "\n",
        "  # Get embeddings model dimension (length of each word vector)\n",
        "  embeddings_dim = embeddings.no_components\n",
        "\n",
        "  # Initialize numpy matrix of zeros as the embeddings matrix\n",
        "  # The dimension will be the number of tokens x the embeddings dimension\n",
        "  embeddings_matrix = np.zeros((num_of_tokens, embeddings_dim))\n",
        "\n",
        "  # Iterate over all words in the word index\n",
        "  for word, i in word_index.items():\n",
        "    # Get the word index in the GloVe dictionary\n",
        "    glove_word_index = embeddings.dictionary.get(word)\n",
        "    if glove_word_index is not None:\n",
        "      # Get the embeddings vector of the corresponding GloVe index\n",
        "      embeddings_vector = embeddings.word_vectors[glove_word_index]\n",
        "      # Set the index of the matrix, because the OOV has the index of 1,\n",
        "      # then the starting index will be i - 1 instead of just i\n",
        "      embeddings_matrix[i - 1] = embeddings_vector\n",
        "\n",
        "  return embeddings_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjbKa4tZSonA"
      },
      "source": [
        "# Generate Training Data from the Sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVnaKirV9sEX"
      },
      "source": [
        "**Generate Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veqKUPt09roq"
      },
      "source": [
        "# Generate all the N-Grams sequences except the last token in each sequences\n",
        "# Simulate predicting the next words available given preceding words\n",
        "# These sliced sequences will be the features of the model\n",
        "def generate_features(n_gram_sequences):\n",
        "  return constant([seq[:-1] for seq in n_gram_sequences])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VBvUnJi_HeU"
      },
      "source": [
        "**Generate Labels**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7FBLAOH_LQT"
      },
      "source": [
        "# Generate all the last token in each N-Grams sequences\n",
        "# Simulate the predicted words given preceding words\n",
        "# Change the labels to be categorical and encode the sequences with One-Hot Encoding\n",
        "# These sliced sequences will be the labels of the model\n",
        "def generate_labels(n_gram_sequences, tokenizer):\n",
        "  # Slice the sequences\n",
        "  labels = [seq[-1] for seq in n_gram_sequences]\n",
        "\n",
        "  # Get the number of tokens to make it the number of classes in the encoding\n",
        "  num_of_tokens = len(tokenizer.word_index)\n",
        "\n",
        "  return to_categorical(labels, num_classes=num_of_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGinLcgtBSWB"
      },
      "source": [
        "# Constructing the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzJfj4hAIVe6"
      },
      "source": [
        "**Construct the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rSUJbOpbFyq"
      },
      "source": [
        "def construct_model(embeddings_matrix, tokenizer, glove, n_units=200, dropout=0.2):\n",
        "  # Get model properties\n",
        "  vocab_size = len(tokenizer.word_index)      # Number of tokens or vocabularies\n",
        "  embeddings_dim = glove.no_components        # The dimension of embeddings matrix vector\n",
        "\n",
        "  # Initialize the Sequential Model\n",
        "  seq_model = Sequential([\n",
        "    # Initialize Ragged Input Layer\n",
        "    InputLayer(input_shape=(None, ), ragged=True),\n",
        "    # Convert the layer into densely-connected layer\n",
        "    tftext.keras.layers.ToDense(pad_value=0, mask=True),\n",
        "    # Inititalize Embedding Layer with weighted embeddings matrix\n",
        "    Embedding(vocab_size, embeddings_dim, embeddings_initializer=Constant(embeddings_matrix), weights=[embeddings_matrix]),\n",
        "    # Dropout Regularization Layer to avoid overfitting\n",
        "    Dropout(dropout),\n",
        "    # Bidirectional LSTM\n",
        "    # Bidirectional here means that the LSTM can do two-way learning\n",
        "    # Return Sequences because there are still many layers needing it\n",
        "    Bidirectional(LSTM(n_units, return_sequences=True)),\n",
        "    # Dropout Regularization\n",
        "    Dropout(dropout),\n",
        "    # One directional LSTM layer\n",
        "    LSTM(n_units),\n",
        "    # Add a dense layer with L2 Regularizer of 0.01\n",
        "    Dense(vocab_size, kernel_regularizer=l2(0.01)),\n",
        "    # Transform the output with Leaky ReLU\n",
        "    LeakyReLU(),\n",
        "    # Add last dense layer with Softmax Activation Function\n",
        "    Dense(vocab_size, activation='softmax')],\n",
        "    # Why not? :D\n",
        "    name=\"Pootry\")\n",
        "\n",
        "  return seq_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPqD5MZbWfNE"
      },
      "source": [
        "**Compile the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TD2smIT6WhTt"
      },
      "source": [
        "# Compile with Adam Optimizer, Categorical Crossentropy loss function, and accuracy metric\n",
        "def compile_model(model, opt='adam', losses='categorical_crossentropy', metric=['accuracy']):\n",
        "  model.compile(loss=losses, optimizer=opt, metrics=metric)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeNO7qbQqyVx"
      },
      "source": [
        "**Show Model Summary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol8E6y6aq0bq"
      },
      "source": [
        "def show_model_summary(model):\n",
        "  model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xE2Q1s2dP9G"
      },
      "source": [
        "**Training the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HKr5rvudSjk"
      },
      "source": [
        "# Train with default number of 100 epochs and 1 verbose\n",
        "def fit_train(model, features, labels, n_epochs=100, verb=1):\n",
        "  model.fit(features, labels, epochs=n_epochs, verbose=verb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYaSsQr3MNg_"
      },
      "source": [
        "**I Know What You Will Say**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6clDR8KTPtm"
      },
      "source": [
        "# Helper\n",
        "# Find words by predicted class index\n",
        "def find_word_by_index(predicted_class, tokenizer):\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == predicted_class:\n",
        "      return word\n",
        "  \n",
        "  return \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ulgwmlKVK7-"
      },
      "source": [
        "# Helper\n",
        "# Output the generated poem\n",
        "def output_poem(pooem):\n",
        "  print(\"Pooem\")\n",
        "  print(\"By: Pootry\\n\")\n",
        "\n",
        "  for verse in pooem[:-1]:\n",
        "    verse[0] = verse[0].capitalize()\n",
        "    verse = \" \".join(verse)\n",
        "    print(verse + \", \")\n",
        "  print((\" \".join(pooem[-1]).capitalize()) + \". \")"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5R7LcfMMVAO"
      },
      "source": [
        "# Making a prediction a.k.a generating new poem from a seed text\n",
        "# Number of verse is the number of verse in the generated poem\n",
        "# Maximum words count is the number of maximum words in each verse\n",
        "# Minimum number words will be maximum words count -5\n",
        "# Minimum value for the maximum words count should be 8\n",
        "def generate_poem(model, seed_text, tokenizer, num_of_verse=8, max_word_count=15):\n",
        "  pooem = []\n",
        "  min_word_count = max_word_count - 5\n",
        "  assert(min_word_count >= 3)\n",
        "\n",
        "  # Cleaning pipeline\n",
        "  seed_text = clean_verses([seed_text])\n",
        "  seed_text = tokenize_verses(seed_text)[0]\n",
        "\n",
        "  # List to contain each line\n",
        "  verse = seed_text.copy()\n",
        "\n",
        "  # Iterate over each verse\n",
        "  for _ in range(num_of_verse):\n",
        "    # Randomly pick number of words in each verses\n",
        "    num_of_words = np.random.randint(min_word_count, max_word_count + 1)\n",
        "\n",
        "    while len(verse) <= num_of_words:\n",
        "      # Convert the seed text to sequences with the tokenizer\n",
        "      seed_sequence = tokenizer.texts_to_sequences(seed_text)[0]\n",
        "      # Convert the sequences into Ragged Tensor\n",
        "      seed_sequence = constant([seed_sequence])\n",
        "      # Predict the word and append it to the seed text and poem\n",
        "      predicted_class = model.predict_classes(seed_sequence, verbose=0)\n",
        "      # Get the corresponding word\n",
        "      predicted_word = find_word_by_index(predicted_class, tokenizer)\n",
        "      verse.append(predicted_word)\n",
        "      \n",
        "      # Append and join all words currently in the seed text\n",
        "      seed_text.append(predicted_word)\n",
        "      seed_text = [\" \".join(seed_text)]\n",
        "    \n",
        "    # Append the verse to the poem list and reset the verse\n",
        "    pooem.append(verse)\n",
        "    verse = []\n",
        "\n",
        "  # Show the poem\n",
        "  output_poem(pooem)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OHx9boDEd1Q"
      },
      "source": [
        "# Gotta Jumble Them All!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0iIBvB6q4vi"
      },
      "source": [
        "# Main function caller\n",
        "def main(data_path, seed_text):\n",
        "  # Read and clean Poem data\n",
        "  verses = read_poems_data(data_path, clean_puncts=True)\n",
        "\n",
        "  # Tokenize the verse to create words embeddings\n",
        "  tokens = tokenize_verses(verses)\n",
        "\n",
        "  # Produce corpus from the tokens\n",
        "  corpus = create_corpus(tokens)\n",
        "\n",
        "  # Construct Tensorflow Keras Tokenizer object\n",
        "  tokenizer = construct_tokenizer(tokens)\n",
        "\n",
        "  # Fit GloVe Embeddings and generate embeddings matrix from the model\n",
        "  glove_embeddings = embed_glove(corpus)\n",
        "  embeddings_matrix = generate_embed_matrix(glove_embeddings, tokenizer)  \n",
        "\n",
        "  # Rejoin sentences from the tokens and create N-Grams Sequences\n",
        "  sentences = rejoin_sentences(tokens)\n",
        "  n_gram_sequences = generate_sequences(tokenizer, sentences)\n",
        "\n",
        "  # Generate features and labels data for training\n",
        "  features = generate_features(n_gram_sequences)\n",
        "  labels = generate_labels(n_gram_sequences, tokenizer)\n",
        "\n",
        "  # Construct the model with given parameters\n",
        "  model = construct_model(embeddings_matrix, tokenizer, glove_embeddings)\n",
        "\n",
        "  # Compile and show the summary of the model\n",
        "  compile_model(model)\n",
        "  model.summary()\n",
        "\n",
        "  # Fit and train the model\n",
        "  fit_train(model, features, labels)\n",
        "\n",
        "  # Generate a poem\n",
        "  generate_poem(model, seed_text, tokenizer)\n",
        "\n",
        "  # Save the model\n",
        "  model.save(\"/content/Pootry\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7VZeuBOsRWD",
        "outputId": "1a946401-da5d-4274-d8f5-4c545a03fe8f"
      },
      "source": [
        "seed_text = \"devil\"\n",
        "main(data_path, seed_text)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"Pootry\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "to_dense (ToDense)           (None, None)              0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, None, 300)         1281000   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, None, 300)         0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, None, 400)         801600    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, None, 400)         0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 200)               480800    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4270)              858270    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 4270)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4270)              18237170  \n",
            "=================================================================\n",
            "Total params: 21,658,840\n",
            "Trainable params: 21,658,840\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "520/520 [==============================] - 167s 310ms/step - loss: 7.1699 - accuracy: 0.0323\n",
            "Epoch 2/100\n",
            "520/520 [==============================] - 161s 309ms/step - loss: 6.6982 - accuracy: 0.0357\n",
            "Epoch 3/100\n",
            "520/520 [==============================] - 160s 307ms/step - loss: 6.5242 - accuracy: 0.0358\n",
            "Epoch 4/100\n",
            "520/520 [==============================] - 160s 308ms/step - loss: 6.3868 - accuracy: 0.0371\n",
            "Epoch 5/100\n",
            "520/520 [==============================] - 160s 307ms/step - loss: 6.2923 - accuracy: 0.0393\n",
            "Epoch 6/100\n",
            "520/520 [==============================] - 160s 307ms/step - loss: 6.1962 - accuracy: 0.0485\n",
            "Epoch 7/100\n",
            "520/520 [==============================] - 163s 314ms/step - loss: 6.0687 - accuracy: 0.0586\n",
            "Epoch 8/100\n",
            "520/520 [==============================] - 159s 306ms/step - loss: 5.9346 - accuracy: 0.0686\n",
            "Epoch 9/100\n",
            "520/520 [==============================] - 161s 309ms/step - loss: 5.7915 - accuracy: 0.0784\n",
            "Epoch 10/100\n",
            "520/520 [==============================] - 159s 306ms/step - loss: 5.6610 - accuracy: 0.0826\n",
            "Epoch 11/100\n",
            "520/520 [==============================] - 160s 307ms/step - loss: 5.5324 - accuracy: 0.0891\n",
            "Epoch 12/100\n",
            "520/520 [==============================] - 161s 309ms/step - loss: 5.4275 - accuracy: 0.0954\n",
            "Epoch 13/100\n",
            "520/520 [==============================] - 161s 309ms/step - loss: 5.3054 - accuracy: 0.1048\n",
            "Epoch 14/100\n",
            "520/520 [==============================] - 158s 304ms/step - loss: 5.1894 - accuracy: 0.1166\n",
            "Epoch 15/100\n",
            "520/520 [==============================] - 156s 300ms/step - loss: 5.0891 - accuracy: 0.1238\n",
            "Epoch 16/100\n",
            "520/520 [==============================] - 157s 301ms/step - loss: 4.9772 - accuracy: 0.1362\n",
            "Epoch 17/100\n",
            "520/520 [==============================] - 156s 301ms/step - loss: 4.8741 - accuracy: 0.1481\n",
            "Epoch 18/100\n",
            "520/520 [==============================] - 156s 300ms/step - loss: 4.7781 - accuracy: 0.1586\n",
            "Epoch 19/100\n",
            "520/520 [==============================] - 156s 300ms/step - loss: 4.6795 - accuracy: 0.1660\n",
            "Epoch 20/100\n",
            "520/520 [==============================] - 155s 297ms/step - loss: 4.5658 - accuracy: 0.1816\n",
            "Epoch 21/100\n",
            "520/520 [==============================] - 156s 300ms/step - loss: 4.4725 - accuracy: 0.1910\n",
            "Epoch 22/100\n",
            "520/520 [==============================] - 156s 299ms/step - loss: 4.3515 - accuracy: 0.2060\n",
            "Epoch 23/100\n",
            "520/520 [==============================] - 157s 301ms/step - loss: 4.2383 - accuracy: 0.2172\n",
            "Epoch 24/100\n",
            "520/520 [==============================] - 159s 305ms/step - loss: 4.1305 - accuracy: 0.2306\n",
            "Epoch 25/100\n",
            "520/520 [==============================] - 159s 306ms/step - loss: 4.0371 - accuracy: 0.2429\n",
            "Epoch 26/100\n",
            "520/520 [==============================] - 161s 309ms/step - loss: 3.9100 - accuracy: 0.2577\n",
            "Epoch 27/100\n",
            "520/520 [==============================] - 162s 310ms/step - loss: 3.7876 - accuracy: 0.2724\n",
            "Epoch 28/100\n",
            "520/520 [==============================] - 161s 310ms/step - loss: 3.6666 - accuracy: 0.2916\n",
            "Epoch 29/100\n",
            "520/520 [==============================] - 160s 308ms/step - loss: 3.5593 - accuracy: 0.3048\n",
            "Epoch 30/100\n",
            "520/520 [==============================] - 160s 307ms/step - loss: 3.4395 - accuracy: 0.3208\n",
            "Epoch 31/100\n",
            "520/520 [==============================] - 160s 308ms/step - loss: 3.3141 - accuracy: 0.3460\n",
            "Epoch 32/100\n",
            "520/520 [==============================] - 160s 308ms/step - loss: 3.1940 - accuracy: 0.3603\n",
            "Epoch 33/100\n",
            "520/520 [==============================] - 161s 309ms/step - loss: 3.0650 - accuracy: 0.3857\n",
            "Epoch 34/100\n",
            "520/520 [==============================] - 161s 310ms/step - loss: 2.9622 - accuracy: 0.4068\n",
            "Epoch 35/100\n",
            "520/520 [==============================] - 161s 310ms/step - loss: 2.8505 - accuracy: 0.4288\n",
            "Epoch 36/100\n",
            "520/520 [==============================] - 160s 308ms/step - loss: 2.7474 - accuracy: 0.4520\n",
            "Epoch 37/100\n",
            "520/520 [==============================] - 160s 308ms/step - loss: 2.6476 - accuracy: 0.4742\n",
            "Epoch 38/100\n",
            "520/520 [==============================] - 161s 309ms/step - loss: 2.5339 - accuracy: 0.4975\n",
            "Epoch 39/100\n",
            "520/520 [==============================] - 161s 309ms/step - loss: 2.4374 - accuracy: 0.5202\n",
            "Epoch 40/100\n",
            "520/520 [==============================] - 162s 311ms/step - loss: 2.3606 - accuracy: 0.5412\n",
            "Epoch 41/100\n",
            "520/520 [==============================] - 161s 310ms/step - loss: 2.2699 - accuracy: 0.5609\n",
            "Epoch 42/100\n",
            "520/520 [==============================] - 160s 308ms/step - loss: 2.1806 - accuracy: 0.5864\n",
            "Epoch 43/100\n",
            "520/520 [==============================] - 160s 307ms/step - loss: 2.1052 - accuracy: 0.5993\n",
            "Epoch 44/100\n",
            "520/520 [==============================] - 160s 308ms/step - loss: 2.0498 - accuracy: 0.6161\n",
            "Epoch 45/100\n",
            "520/520 [==============================] - 158s 304ms/step - loss: 1.9767 - accuracy: 0.6346\n",
            "Epoch 46/100\n",
            "520/520 [==============================] - 160s 307ms/step - loss: 1.9262 - accuracy: 0.6492\n",
            "Epoch 47/100\n",
            "520/520 [==============================] - 160s 308ms/step - loss: 1.8583 - accuracy: 0.6630\n",
            "Epoch 48/100\n",
            "520/520 [==============================] - 159s 306ms/step - loss: 1.8094 - accuracy: 0.6740\n",
            "Epoch 49/100\n",
            "520/520 [==============================] - 159s 306ms/step - loss: 1.7616 - accuracy: 0.6881\n",
            "Epoch 50/100\n",
            "520/520 [==============================] - 160s 307ms/step - loss: 1.7016 - accuracy: 0.7000\n",
            "Epoch 51/100\n",
            "520/520 [==============================] - 161s 309ms/step - loss: 1.6686 - accuracy: 0.7045\n",
            "Epoch 52/100\n",
            "520/520 [==============================] - 160s 308ms/step - loss: 1.6371 - accuracy: 0.7122\n",
            "Epoch 53/100\n",
            "520/520 [==============================] - 159s 306ms/step - loss: 1.6109 - accuracy: 0.7180\n",
            "Epoch 54/100\n",
            "520/520 [==============================] - 159s 305ms/step - loss: 1.5550 - accuracy: 0.7270\n",
            "Epoch 55/100\n",
            "520/520 [==============================] - 159s 306ms/step - loss: 1.5185 - accuracy: 0.7356\n",
            "Epoch 56/100\n",
            "520/520 [==============================] - 158s 304ms/step - loss: 1.5092 - accuracy: 0.7385\n",
            "Epoch 57/100\n",
            "520/520 [==============================] - 159s 305ms/step - loss: 1.4760 - accuracy: 0.7450\n",
            "Epoch 58/100\n",
            "520/520 [==============================] - 160s 307ms/step - loss: 1.4494 - accuracy: 0.7500\n",
            "Epoch 59/100\n",
            "520/520 [==============================] - 159s 306ms/step - loss: 1.4148 - accuracy: 0.7579\n",
            "Epoch 60/100\n",
            "520/520 [==============================] - 159s 306ms/step - loss: 1.3757 - accuracy: 0.7614\n",
            "Epoch 61/100\n",
            "520/520 [==============================] - 159s 306ms/step - loss: 1.3789 - accuracy: 0.7619\n",
            "Epoch 62/100\n",
            "520/520 [==============================] - 160s 307ms/step - loss: 1.3540 - accuracy: 0.7669\n",
            "Epoch 63/100\n",
            "520/520 [==============================] - 159s 305ms/step - loss: 1.3256 - accuracy: 0.7739\n",
            "Epoch 64/100\n",
            "520/520 [==============================] - 159s 306ms/step - loss: 1.3218 - accuracy: 0.7730\n",
            "Epoch 65/100\n",
            "520/520 [==============================] - 159s 306ms/step - loss: 1.2979 - accuracy: 0.7802\n",
            "Epoch 66/100\n",
            "520/520 [==============================] - 160s 307ms/step - loss: 1.2780 - accuracy: 0.7816\n",
            "Epoch 67/100\n",
            "520/520 [==============================] - 161s 308ms/step - loss: 1.2663 - accuracy: 0.7830\n",
            "Epoch 68/100\n",
            "520/520 [==============================] - 160s 308ms/step - loss: 1.2524 - accuracy: 0.7836\n",
            "Epoch 69/100\n",
            "520/520 [==============================] - 160s 308ms/step - loss: 1.2382 - accuracy: 0.7843\n",
            "Epoch 70/100\n",
            "520/520 [==============================] - 161s 310ms/step - loss: 1.2204 - accuracy: 0.7929\n",
            "Epoch 71/100\n",
            "520/520 [==============================] - 161s 309ms/step - loss: 1.2140 - accuracy: 0.7936\n",
            "Epoch 72/100\n",
            "520/520 [==============================] - 161s 310ms/step - loss: 1.2192 - accuracy: 0.7889\n",
            "Epoch 73/100\n",
            "520/520 [==============================] - 160s 308ms/step - loss: 1.1740 - accuracy: 0.7992\n",
            "Epoch 74/100\n",
            "520/520 [==============================] - 160s 308ms/step - loss: 1.1559 - accuracy: 0.8028\n",
            "Epoch 75/100\n",
            "520/520 [==============================] - 163s 313ms/step - loss: 1.1636 - accuracy: 0.7957\n",
            "Epoch 76/100\n",
            "520/520 [==============================] - 161s 310ms/step - loss: 1.1945 - accuracy: 0.7942\n",
            "Epoch 77/100\n",
            "520/520 [==============================] - 160s 308ms/step - loss: 1.1399 - accuracy: 0.8019\n",
            "Epoch 78/100\n",
            "520/520 [==============================] - 160s 308ms/step - loss: 1.1386 - accuracy: 0.7986\n",
            "Epoch 79/100\n",
            "520/520 [==============================] - 160s 308ms/step - loss: 1.1259 - accuracy: 0.8047\n",
            "Epoch 80/100\n",
            "520/520 [==============================] - 161s 308ms/step - loss: 1.1242 - accuracy: 0.8041\n",
            "Epoch 81/100\n",
            "520/520 [==============================] - 160s 308ms/step - loss: 1.1138 - accuracy: 0.8039\n",
            "Epoch 82/100\n",
            "520/520 [==============================] - 161s 308ms/step - loss: 1.1045 - accuracy: 0.8080\n",
            "Epoch 83/100\n",
            "520/520 [==============================] - 160s 308ms/step - loss: 1.0942 - accuracy: 0.8097\n",
            "Epoch 84/100\n",
            "520/520 [==============================] - 161s 309ms/step - loss: 1.0996 - accuracy: 0.8092\n",
            "Epoch 85/100\n",
            "520/520 [==============================] - 162s 312ms/step - loss: 1.0950 - accuracy: 0.8088\n",
            "Epoch 86/100\n",
            "520/520 [==============================] - 160s 308ms/step - loss: 1.0578 - accuracy: 0.8139\n",
            "Epoch 87/100\n",
            "520/520 [==============================] - 161s 310ms/step - loss: 1.0543 - accuracy: 0.8157\n",
            "Epoch 88/100\n",
            "520/520 [==============================] - 160s 308ms/step - loss: 1.0551 - accuracy: 0.8122\n",
            "Epoch 89/100\n",
            "520/520 [==============================] - 160s 308ms/step - loss: 1.0604 - accuracy: 0.8139\n",
            "Epoch 90/100\n",
            "520/520 [==============================] - 160s 307ms/step - loss: 1.0643 - accuracy: 0.8117\n",
            "Epoch 91/100\n",
            "520/520 [==============================] - 159s 305ms/step - loss: 1.0363 - accuracy: 0.8168\n",
            "Epoch 92/100\n",
            "520/520 [==============================] - 159s 306ms/step - loss: 1.0478 - accuracy: 0.8129\n",
            "Epoch 93/100\n",
            "520/520 [==============================] - 158s 304ms/step - loss: 1.0275 - accuracy: 0.8159\n",
            "Epoch 94/100\n",
            "520/520 [==============================] - 159s 306ms/step - loss: 1.0306 - accuracy: 0.8164\n",
            "Epoch 95/100\n",
            "520/520 [==============================] - 158s 304ms/step - loss: 1.0284 - accuracy: 0.8163\n",
            "Epoch 96/100\n",
            "520/520 [==============================] - 160s 307ms/step - loss: 1.0552 - accuracy: 0.8135\n",
            "Epoch 97/100\n",
            "520/520 [==============================] - 160s 307ms/step - loss: 1.0091 - accuracy: 0.8188\n",
            "Epoch 98/100\n",
            "520/520 [==============================] - 159s 306ms/step - loss: 1.0026 - accuracy: 0.8202\n",
            "Epoch 99/100\n",
            "520/520 [==============================] - 159s 306ms/step - loss: 0.9924 - accuracy: 0.8217\n",
            "Epoch 100/100\n",
            "520/520 [==============================] - 159s 306ms/step - loss: 0.9736 - accuracy: 0.8245\n",
            "Devil of of of of of of of of of of of, \n",
            "Of of of of of of of of of of of of, \n",
            "Of of of of of of of of of of of of of of of, \n",
            "Of of of of of of of of of of of of of, \n",
            "Of of of of of of of of of of of of of of of of, \n",
            "Of of of of of of of of of of of of, \n",
            "Of of of of of of of of of of of of of of of, \n",
            "of of of of of of of of of of of of of of of of. \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Function `_wrapped_model` contains input name(s) args_0 with unsupported characters which will be renamed to args_0_1 in the SavedModel.\n",
            "WARNING:absl:Found untraced functions such as masking_layer_call_and_return_conditional_losses, masking_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, masking_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/Pootry/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/Pootry/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAO1hQl4-TrO"
      },
      "source": [
        "# Oopsie, I have a really long short-term memory there.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Apkvmpau-YTE"
      },
      "source": [
        "No problem, having it saved sure ease my mind. :D Anyway, there are code changes made after the training, ignoring warnings and changing those print test for example, andddd, some secrets of course.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEZ1mwH84UH5",
        "outputId": "09823d67-8f05-44a2-eb6c-cbd302b78c84"
      },
      "source": [
        "# Loading the model\n",
        "# The saved model can be found in the same repository\n",
        "# Wait nope, not a good idea. The link to the model can be accessed here\n",
        "# https://drive.google.com/drive/folders/1Qq7jSFCAGKGZdXy2MJcXOt9BvKRcdXr0?usp=sharing\n",
        "model = load_model(\"Pootry\")\n",
        "model.summary()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"Pootry\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "to_dense (ToDense)           (None, None)              0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, None, 300)         1281000   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, None, 300)         0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, None, 400)         801600    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, None, 400)         0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 200)               480800    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4270)              858270    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 4270)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4270)              18237170  \n",
            "=================================================================\n",
            "Total params: 21,658,840\n",
            "Trainable params: 21,658,840\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiY5xpgt7FR0",
        "outputId": "b4eed90b-a3bf-4a56-a9e8-0ac774c1c8e6"
      },
      "source": [
        "# Poem #1\n",
        "seed_text = \"revolts, mutinies, and multitude\"\n",
        "generate_poem(model, seed_text, tokenizer)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pooem\n",
            "By: Pootry\n",
            "\n",
            "Revolts mutinies and multitude thus i' his fawn and love to, \n",
            "Richard so hast in yours for this battle pleased sights of men's right, \n",
            "Of mine right happy son their happy son their death aches, \n",
            "Last side straight happy to Richard hence him go in me establish'd there see thee false, \n",
            "Glass to Clarence these wars in peace aches vile was heaven either thou eyes confound victorious, \n",
            "Wreaths herself of faded lies your power upon heaven shame me him he was sorrow people, \n",
            "There will despair cry carnal cur lose hell right again lose dangerous thing more sleep upon, \n",
            "Fear me where spake sport will weeds revenge this death go in me attend my. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5RDezE4-NND",
        "outputId": "6252a51f-2a00-4af2-cc39-ee3392f8d579"
      },
      "source": [
        "# Poem #2\n",
        "seed_text = \"manacles tied my mind\"\n",
        "generate_poem(model, seed_text, tokenizer)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pooem\n",
            "By: Pootry\n",
            "\n",
            "Manacles tied my mind march by his wall by birth me in your spears Why, \n",
            "None more ever redemption plebeians 'we have shalt fetch his queen of uncle him, \n",
            "Richard there looks men it be groans stood offer'd along up me, \n",
            "Strew'd upon enemy love him spake for Englishmen fill'd and offer'd dreams 'we point of, \n",
            "Accomplish dream will power happy and power courageous friends pride Richard leaves heaven go, \n",
            "Upon me up him upon thirst the king Volsces side hath fond, \n",
            "For heaven am made queen me aches fame thou hast and heaven live thee power, \n",
            "Why was unto my heart edward against a face against it. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_shmb8iLASxC",
        "outputId": "4ad36072-927c-4cd4-e676-9dc708de909e"
      },
      "source": [
        "# Poem #3\n",
        "seed_text = \"in front of shunless destiny!\"\n",
        "generate_poem(model, seed_text, tokenizer)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pooem\n",
            "By: Pootry\n",
            "\n",
            "In front of shunless destiny me hath spoken groans lets me O hope, \n",
            "Was thy death windows will faded 'we have shalt fetch his queen of sorrow walls us, \n",
            "Say fresher amen part heaven speak him hither with either power, \n",
            "Richard outweighs trouble us use it to thy rage heaven his zeal kind of, \n",
            "Evil confound Guilty guilty of yours will thou affairs affairs born, \n",
            "'we have shalt Rome like peace flower wall on me shall, \n",
            "Mothers' sons and men count peace happy and right power hereafter thy, \n",
            "Dearest mind of wrongs breathest infidels guilty of brothers and power foot was. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g71N61tmAcBT",
        "outputId": "949d162a-9c36-41f1-8c05-05a855b637fb"
      },
      "source": [
        "# Poem #4\n",
        "seed_text = \"Heaven plagues thee.\"\n",
        "generate_poem(model, seed_text, tokenizer)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pooem\n",
            "By: Pootry\n",
            "\n",
            "Heaven plagues thee common pleasing of love eyes his joints camel camel Richard outweighs fortunes in, \n",
            "My wives of enemy fathers up me affrights you spake to see me about it right, \n",
            "Out of fear fear right weeping blood right of battle looks looks in, \n",
            "Weeping time a king faded aches in battle pleased how thou yours for things, \n",
            "Miles right enemy peace nail these wars and comest Trust camel, \n",
            "Camel proportion me who's so fortunes to thee in fear withal those, \n",
            "Cobham to fetch his queen of uncle him Richard there looks looks, \n",
            "Hast prey to things we fall it true affairs hast guilty odds. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8XAOxqsAyzc",
        "outputId": "265e3471-d0f2-4af0-e57a-1e7d4288c5ad"
      },
      "source": [
        "# Poem #5\n",
        "seed_text = \"<slander him who?> \"\n",
        "generate_poem(model, seed_text, tokenizer)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pooem\n",
            "By: Pootry\n",
            "\n",
            "Slander him who of a name like fathers unto love why man then have, \n",
            "Peace and true name of battle see to shame King these battle looks will groans looks, \n",
            "Swears then hath broken in the world 'we have shalt reap his time fierce blest, \n",
            "That weeping gains blest offer'd offer'd blown and lies your zeal there saved, \n",
            "Advised respect how cur upon Richard' lies heaven them stamp upon your power about like Tewksbury, \n",
            "Strength be breathest how lose life horse Hastings in your uncle crying pride, \n",
            "Guilty of enemy Richard outweighs enemy offer'd allies men's right of enemy, \n",
            "Right men suit faded thou uncle men well comest adder alive. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzL3wr8wBDrX"
      },
      "source": [
        "The last surely is the **best** one. :DDDDDDDDDDDDD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNHHWhZiBMAC"
      },
      "source": [
        "# Takeaways"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxUbj_ZNDX6Z"
      },
      "source": [
        "Playing with texts in NLP sure is fun. There are many challenges I had when using this new architecture for me, and it was quite an adventurous exploration, digging in GloVe Embeddings and Ragged Tensors. The model has the accuracy of more than 80% which is somehow great, but improvement on its grammar and tagging all tokens' parts of speech might produce better results. 21 million of parameters sure is huge. :D"
      ]
    }
  ]
}